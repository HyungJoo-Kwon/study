{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dataset 클래스와 전처리\n",
        "if '__file__' in globals():\n",
        "    import os, sys\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, train=True):\n",
        "        self.train = train\n",
        "        self.data = None\n",
        "        self.label = None\n",
        "        self.prepare()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        assert np.isscalar(index)\n",
        "        if self.label is None:\n",
        "            return self.data[index], None\n",
        "        else:\n",
        "            return self.data[index], self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def prepare(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import dezero\n",
        "import dezero.functions as F\n",
        "from dezero import optimizers\n",
        "from dezero.models import MLP\n",
        "\n",
        "\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "lr = 1.0\n",
        "\n",
        "train_set = dezero.datasets.Spiral(train=True)\n",
        "model = MLP((hidden_size, 3))\n",
        "optimizer = optimizers.SGD(lr).setup(model)\n",
        "\n",
        "data_size = len(train_set)\n",
        "max_iter = math.ceil(data_size / batch_size)\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    # Shuffle index for data\n",
        "    index = np.random.permutation(data_size)\n",
        "    sum_loss = 0\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Create minibatch\n",
        "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
        "        batch = [train_set[i] for i in batch_index]\n",
        "        batch_x = np.array([example[0] for example in batch])\n",
        "        batch_t = np.array([example[1] for example in batch])\n",
        "\n",
        "        y = model(batch_x)\n",
        "        loss = F.softmax_cross_entropy(y, batch_t)\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "        optimizer.update()\n",
        "\n",
        "        sum_loss += float(loss.data) * len(batch_t)\n",
        "\n",
        "    # Print loss every epoch\n",
        "    avg_loss = sum_loss / data_size\n",
        "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}